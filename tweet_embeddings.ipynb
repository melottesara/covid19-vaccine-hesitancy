{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spare-workplace",
   "metadata": {},
   "source": [
    "# Build Data Representations \n",
    "### text only, text + hashtags, and hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-animation",
   "metadata": {},
   "source": [
    "It is assumed that a master csv file (named \"master_individual.csv\") exists containing the following information for each individual tweet:\n",
    "- tweet ID, full text, sentiment score, date, hashtags\n",
    "- location of origin data (city, state, place type, zip code, metropolitan area)\n",
    "- data for the zip code from which the tweet originates (average Zillow Home Value Index (ZHVI), number of establishments in educational services, number of establishments in healthcare and social assistance, number of establishments in professional, scientific, and technical services, ground truth vaccine hesitancy, binarized ground truth vaccine hesitancy)\n",
    "\n",
    "Note: When binarizing the ground truth vaccine hesitancy for each zip code, we used 0.70 as the cut-off (i.e. a continuous ground truth vaccine hesitancy of >= 0.70 corresponds to a 1 and all other cases results in 0). However, binarized ground truth vaccine hesitancy is ultimately not used in study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "running-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gilbert",
   "metadata": {},
   "source": [
    "The full_text attribute contains the entire tweet: text + hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "atomic-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 16)\n"
     ]
    }
   ],
   "source": [
    "master_individual = pd.read_csv('master_individual.csv')\n",
    "print(master_individual.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-brother",
   "metadata": {},
   "source": [
    "# 1) Count number of tweets and hashtags before initial text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-temperature",
   "metadata": {},
   "source": [
    "### 1.1) Number of tweets in each metropolitan area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "metric-facility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:\n",
      "New York: (12612, 3)\n",
      "Los Angeles: (10532, 3)\n",
      "Chicago: (1544, 3)\n",
      "Houston: (1529, 3)\n",
      "San Diego: (1061, 3)\n",
      "Philadelphia: (817, 3)\n",
      "Dallas: (664, 3)\n",
      "Phoenix: (496, 3)\n",
      "San Antonio: (203, 3)\n"
     ]
    }
   ],
   "source": [
    "num_hashtags_df = pd.DataFrame()\n",
    "num_hashtags_df['full_text'] = master_individual['full_text'].values\n",
    "num_hashtags_df['hashtags'] = master_individual['hashtags'].values\n",
    "num_hashtags_df['metropolitan_area'] = master_individual['metropolitan_area'].values\n",
    "\n",
    "newyork = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'NewYork']\n",
    "newyork.reset_index(drop=True, inplace=True)\n",
    "\n",
    "losangeles = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'LosAngeles']\n",
    "losangeles.reset_index(drop=True, inplace=True)\n",
    "\n",
    "chicago = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Chicago']\n",
    "chicago.reset_index(drop=True, inplace=True)\n",
    "\n",
    "houston = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Houston']\n",
    "houston.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sandiego = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'SanDiego']\n",
    "sandiego.reset_index(drop=True, inplace=True)\n",
    "\n",
    "philadelphia = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Philadelphia']\n",
    "philadelphia.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dallas = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Dallas']\n",
    "dallas.reset_index(drop=True, inplace=True)\n",
    "\n",
    "phoenix = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Phoenix']\n",
    "phoenix.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sanantonio = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'SanAntonio']\n",
    "sanantonio.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Number of tweets:')\n",
    "print('New York:', newyork.shape)\n",
    "print('Los Angeles:', losangeles.shape)\n",
    "print('Chicago:', chicago.shape)\n",
    "print('Houston:', houston.shape)\n",
    "print('San Diego:', sandiego.shape)\n",
    "print('Philadelphia:', philadelphia.shape)\n",
    "print('Dallas:', dallas.shape)\n",
    "print('Phoenix:', phoenix.shape)\n",
    "print('San Antonio:', sanantonio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-configuration",
   "metadata": {},
   "source": [
    "### 1.2) Number of hashtags in each metropolitan area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "affecting-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count hashtags by looking at how many '#' appear in full text\n",
    "def count_hashtags(df):\n",
    "    tokenized_df = df.split()\n",
    "    count = 0\n",
    "    for item in tokenized_df:\n",
    "        # check if # is in the item as opposed to the first character\n",
    "        # because if the # immediately follows an emoji, for instance, then\n",
    "        # it won't be counted (even though it should)\n",
    "        if '#' in item:\n",
    "            for character in item:\n",
    "                if character == '#':\n",
    "                    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "collaborative-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num hashtags New York: 41232\n",
      "total num hashtags Los Angeles: 37030\n",
      "total num hashtags Chicago: 3857\n",
      "total num hashtags Houston: 5557\n",
      "total num hashtags San Diego: 3019\n",
      "total num hashtags Philadelphia: 2753\n",
      "total num hashtags Dallas: 2250\n",
      "total num hashtags Phoenix: 1576\n",
      "total num hashtags San Antonio: 765\n"
     ]
    }
   ],
   "source": [
    "print('total num hashtags New York:', newyork['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Los Angeles:', losangeles['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Chicago:', chicago['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Houston:', houston['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags San Diego:', sandiego['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Philadelphia:', philadelphia['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Dallas:', dallas['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags Phoenix:', phoenix['full_text'].apply(count_hashtags).sum())\n",
    "print('total num hashtags San Antonio:', sanantonio['full_text'].apply(count_hashtags).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-savannah",
   "metadata": {},
   "source": [
    "# 2) Initial text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-spiritual",
   "metadata": {},
   "source": [
    "Check if any tweets are retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "superior-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retweets: 0\n"
     ]
    }
   ],
   "source": [
    "num_retweets = len(master_individual[master_individual['full_text'].str.startswith('RT')])\n",
    "print('Number of retweets:', num_retweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-watts",
   "metadata": {},
   "source": [
    "Tokenize full text, make lower case, and remove mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "therapeutic-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=False, strip_handles=True)\n",
    "master_individual['processed_full_text'] = master_individual['full_text'].apply(tweet_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-madness",
   "metadata": {},
   "source": [
    "Remove URLs, stop words, tokens of length <= 1, any characters other than letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "subtle-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_information(df):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('not')\n",
    "    stop_words.remove('no')\n",
    "    stop_words.remove('nor')\n",
    "    stop_words.remove('very')\n",
    "    stop_words.remove('most')\n",
    "    \n",
    "    new_tweet = []\n",
    "    \n",
    "    for token in df:\n",
    "        \n",
    "        if token[0:4] == 'http':\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        if token == '' or token == ' ' or len(token) == 1:\n",
    "            continue\n",
    "        \n",
    "        token = re.sub(r'[^a-zA-Z#]', '', token)\n",
    "        token_list = token.split()\n",
    "        \n",
    "        if token_list:\n",
    "            for item in token_list:\n",
    "                if item in stop_words:\n",
    "                    continue\n",
    "                if item == '' or item == ' ' or len(item) == 1:\n",
    "                    continue\n",
    "                new_tweet.append(item)\n",
    "    \n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "satellite-difference",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_individual['processed_full_text'] = master_individual['processed_full_text'].apply(remove_extra_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-minneapolis",
   "metadata": {},
   "source": [
    "# 3) Count number of tweets and hashtags after initial text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-democracy",
   "metadata": {},
   "source": [
    "### 3.1) Number of tweets in each metropolitan area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dangerous-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:\n",
      "New York: (12612, 3)\n",
      "Los Angeles: (10532, 3)\n",
      "Chicago: (1544, 3)\n",
      "Houston: (1529, 3)\n",
      "San Diego: (1061, 3)\n",
      "Philadelphia: (817, 3)\n",
      "Dallas: (664, 3)\n",
      "Phoenix: (496, 3)\n",
      "San Antonio: (203, 3)\n"
     ]
    }
   ],
   "source": [
    "num_hashtags_df = pd.DataFrame()\n",
    "num_hashtags_df['full_text'] = master_individual['full_text'].values\n",
    "num_hashtags_df['processed_full_text'] = master_individual['processed_full_text'].values\n",
    "num_hashtags_df['metropolitan_area'] = master_individual['metropolitan_area'].values\n",
    "\n",
    "newyork = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'NewYork']\n",
    "newyork.reset_index(drop=True, inplace=True)\n",
    "\n",
    "losangeles = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'LosAngeles']\n",
    "losangeles.reset_index(drop=True, inplace=True)\n",
    "\n",
    "chicago = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Chicago']\n",
    "chicago.reset_index(drop=True, inplace=True)\n",
    "\n",
    "houston = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Houston']\n",
    "houston.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sandiego = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'SanDiego']\n",
    "sandiego.reset_index(drop=True, inplace=True)\n",
    "\n",
    "philadelphia = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Philadelphia']\n",
    "philadelphia.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dallas = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Dallas']\n",
    "dallas.reset_index(drop=True, inplace=True)\n",
    "\n",
    "phoenix = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'Phoenix']\n",
    "phoenix.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sanantonio = num_hashtags_df[num_hashtags_df['metropolitan_area'] == 'SanAntonio']\n",
    "sanantonio.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Number of tweets:')\n",
    "print('New York:', newyork.shape)\n",
    "print('Los Angeles:', losangeles.shape)\n",
    "print('Chicago:', chicago.shape)\n",
    "print('Houston:', houston.shape)\n",
    "print('San Diego:', sandiego.shape)\n",
    "print('Philadelphia:', philadelphia.shape)\n",
    "print('Dallas:', dallas.shape)\n",
    "print('Phoenix:', phoenix.shape)\n",
    "print('San Antonio:', sanantonio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-apartment",
   "metadata": {},
   "source": [
    "### 3.2) Number of hashtags in each metropolitan area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "closed-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of hashtags\n",
    "def get_num_hashtags(df):\n",
    "    hashtag_count = 0\n",
    "    unique_hashtags = set()\n",
    "    for index, row in df.iterrows():\n",
    "        count, unique_hashtags = count_hashtags(row['processed_full_text'], unique_hashtags)\n",
    "        hashtag_count += count\n",
    "    return hashtag_count, len(unique_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "verbal-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of hashtags and add unique hashtags to set\n",
    "def count_hashtags(full_text, unique_hashtag_set):\n",
    "    hashtag_count = 0\n",
    "    for token in full_text:\n",
    "        if token[0] == '#':\n",
    "            hashtag_count += 1\n",
    "            unique_hashtag_set.add(token)\n",
    "    return hashtag_count, unique_hashtag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "little-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(total num hashtags, num unique hashtags) New York: (40419, 10764)\n",
      "(total num hashtags, num unique hashtags) Los Angeles: (36507, 12422)\n",
      "(total num hashtags, num unique hashtags) Chicago: (3792, 1891)\n",
      "(total num hashtags, num unique hashtags) Houston: (5505, 2526)\n",
      "(total num hashtags, num unique hashtags) San Diego: (2980, 1769)\n",
      "(total num hashtags, num unique hashtags) Philadelphia: (2727, 1349)\n",
      "(total num hashtags, num unique hashtags) Dallas: (2231, 1211)\n",
      "(total num hashtags, num unique hashtags) Phoenix: (1563, 1003)\n",
      "(total num hashtags, num unique hashtags) San Antonio: (756, 340)\n"
     ]
    }
   ],
   "source": [
    "print('(total num hashtags, num unique hashtags) New York:', get_num_hashtags(newyork))\n",
    "print('(total num hashtags, num unique hashtags) Los Angeles:', get_num_hashtags(losangeles))\n",
    "print('(total num hashtags, num unique hashtags) Chicago:', get_num_hashtags(chicago))\n",
    "print('(total num hashtags, num unique hashtags) Houston:', get_num_hashtags(houston))\n",
    "print('(total num hashtags, num unique hashtags) San Diego:', get_num_hashtags(sandiego))\n",
    "print('(total num hashtags, num unique hashtags) Philadelphia:', get_num_hashtags(philadelphia))\n",
    "print('(total num hashtags, num unique hashtags) Dallas:', get_num_hashtags(dallas))\n",
    "print('(total num hashtags, num unique hashtags) Phoenix:', get_num_hashtags(phoenix))\n",
    "print('(total num hashtags, num unique hashtags) San Antonio:', get_num_hashtags(sanantonio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-cradle",
   "metadata": {},
   "source": [
    "# 4) Build data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-carry",
   "metadata": {},
   "source": [
    "### 4.1) Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "mature-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(df):\n",
    "    new_tweet = []\n",
    "    for token in df:\n",
    "        if token[0] == '#':\n",
    "            continue\n",
    "        new_tweet.append(token)\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ranging-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df):\n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fantastic-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tweet_text = []\n",
    "    for token in df:\n",
    "        new_tweet_text.append(lemmatizer.lemmatize(token))\n",
    "    return new_tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "conservative-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_string(df):\n",
    "    return ' '.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "convenient-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 5)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "master_individual_copy = master_individual.copy()\n",
    "\n",
    "text_only_df = pd.DataFrame()\n",
    "text_only_df['metropolitan_area'] = master_individual_copy['metropolitan_area']\n",
    "text_only_df['zip_code'] = master_individual_copy['zip_code']\n",
    "text_only_df['vac_hes_bin'] = master_individual_copy['vac_hes_bin']\n",
    "\n",
    "text_only_df['processed_tweet'] = master_individual_copy['processed_full_text'].apply(remove_hashtags)\n",
    "text_only_df['num_words'] = text_only_df['processed_tweet'].apply(count_words)\n",
    "text_only_df['processed_tweet'] = text_only_df['processed_tweet'].apply(lemmatize_text)\n",
    "text_only_df['processed_tweet'] = text_only_df['processed_tweet'].apply(create_full_string)\n",
    "\n",
    "print(text_only_df.shape)\n",
    "print(text_only_df['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "transparent-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_vac_hes = (text_only_df.groupby('vac_hes_bin')['num_words'].apply(lambda x: np.mean(x)).reset_index(name='avg_len_tweet'))\n",
    "group_by_met_area = (text_only_df.groupby('metropolitan_area')['num_words'].apply(lambda x: np.mean(x))).reset_index(name='avg_len_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-hands",
   "metadata": {},
   "source": [
    "### 4.2) Text + hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "handy-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pound_symbol(df):\n",
    "    new_tweet = []\n",
    "    for token in df:\n",
    "        if token[0] == '#':\n",
    "            token = token[1:]\n",
    "        new_tweet.append(token)\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "straight-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 5)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "master_individual_copy = master_individual.copy()\n",
    "\n",
    "text_and_hashtags_df = pd.DataFrame()\n",
    "text_and_hashtags_df['metropolitan_area'] = master_individual_copy['metropolitan_area']\n",
    "text_and_hashtags_df['zip_code'] = master_individual_copy['zip_code']\n",
    "text_and_hashtags_df['vac_hes_bin'] = master_individual_copy['vac_hes_bin']\n",
    "\n",
    "text_and_hashtags_df['processed_tweet'] = master_individual_copy['processed_full_text'].apply(remove_pound_symbol)\n",
    "text_and_hashtags_df['processed_tweet'] = text_and_hashtags_df['processed_tweet'].apply(lemmatize_text)\n",
    "text_and_hashtags_df['num_words'] = text_and_hashtags_df['processed_tweet'].apply(count_words)\n",
    "text_and_hashtags_df['processed_tweet'] = text_and_hashtags_df['processed_tweet'].apply(create_full_string)\n",
    "\n",
    "print(text_and_hashtags_df.shape)\n",
    "print(text_and_hashtags_df['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "premier-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_vac_hes = (text_and_hashtags_df.groupby('vac_hes_bin')['num_words'].apply(lambda x: np.mean(x)).reset_index(name='avg_len_tweet'))\n",
    "group_by_met_area = (text_and_hashtags_df.groupby('metropolitan_area')['num_words'].apply(lambda x: np.mean(x))).reset_index(name='avg_len_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-irrigation",
   "metadata": {},
   "source": [
    "### 4.3) Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "metallic-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags_and_words(df):\n",
    "    tokenized_hashtags = df.split()\n",
    "    return len(tokenized_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "southern-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_hashtags_and_words(df):\n",
    "    tokenized_hashtags = df.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tweet_text = []\n",
    "    for token in tokenized_hashtags:\n",
    "        new_tweet_text.append(lemmatizer.lemmatize(token))\n",
    "    return ' '.join(new_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "severe-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 5)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "master_individual_copy = master_individual.copy()\n",
    "\n",
    "tweet_list = []\n",
    "for index, row in master_individual_copy.iterrows():\n",
    "    \n",
    "    hashtags = row['hashtags']\n",
    "    full_text_list = row['processed_full_text']\n",
    "    new_tweet = []\n",
    "    \n",
    "    if hashtags == '[]':\n",
    "        # use full text\n",
    "        tweet_list.append(' '.join(full_text_list))\n",
    "    else:\n",
    "        # use hashtags\n",
    "        hashtags_list = []\n",
    "        for token in full_text_list:\n",
    "            if token[0] == '#':\n",
    "                hashtags_list.append(token[1:])\n",
    "        tweet_list.append(' '.join(hashtags_list))\n",
    "\n",
    "hybrid_df = pd.DataFrame(tweet_list, columns=['processed_tweet'])\n",
    "hybrid_df['processed_tweet'] = hybrid_df['processed_tweet'].apply(lemmatize_hashtags_and_words)\n",
    "hybrid_df['num_tokens'] = hybrid_df['processed_tweet'].apply(count_hashtags_and_words)\n",
    "\n",
    "hybrid_df['metropolitan_area'] = master_individual_copy['metropolitan_area']\n",
    "hybrid_df['zip_code'] = master_individual_copy['zip_code']\n",
    "hybrid_df['vac_hes_bin'] = master_individual_copy['vac_hes_bin']\n",
    "\n",
    "print(hybrid_df.shape)\n",
    "print(hybrid_df['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "rolled-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_vac_hes = (hybrid_df.groupby('vac_hes_bin')['num_tokens'].apply(lambda x: np.mean(x)).reset_index(name='avg_len_tweet'))\n",
    "group_by_met_area = (hybrid_df.groupby('metropolitan_area')['num_tokens'].apply(lambda x: np.mean(x))).reset_index(name='avg_len_tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-wilderness",
   "metadata": {},
   "source": [
    "# 5) Embed tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-carroll",
   "metadata": {},
   "source": [
    "### 5.1) Load pre-trained model from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "turkish-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model_path='/Volumes/More Memory/Covid-19 Project Data/wiki/wiki.en.bin'):\n",
    "    return fasttext.load_model(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "hourly-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "instructional-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize tweets using fastText pre-trained model\n",
    "def vectorize_tweets(tweets_list):\n",
    "    vectors = []\n",
    "    for tweet in tweets_list:\n",
    "        vectors.append(model.get_sentence_vector(tweet))\n",
    "    return np.asarray(vectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "thick-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create headers of dimensions in dataframe\n",
    "def create_dimensions_array():\n",
    "    values = np.arange(0, 300, 1)\n",
    "    dimensions = []\n",
    "    for dim in values:\n",
    "        dimensions.append('dim_' + str(dim))\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "attended-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add additional information (tweet-level and zip code-level) to dataframe \n",
    "def add_additional_information(df):\n",
    "    df['id'] = master_individual['id'].values\n",
    "    df['sentiment'] = master_individual['sentiment'].values\n",
    "    df['zip_code'] = master_individual['zip_code'].values\n",
    "    df['metropolitan_area'] = master_individual['metropolitan_area'].values\n",
    "    df['avg_zhvi'] = master_individual['avg_zhvi'].values\n",
    "    df['num_est_educ_serv'] = master_individual['num_est_educ_serv'].values\n",
    "    df['num_est_healthcare_social_assist'] = master_individual['num_est_healthcare_social_assist'].values\n",
    "    df['num_est_prof_sci_tech_serv'] = master_individual['num_est_prof_sci_tech_serv'].values\n",
    "    df['vac_hes'] = master_individual['vac_hes'].values\n",
    "    df['vac_hes_bin'] = master_individual['vac_hes_bin'].values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-cooking",
   "metadata": {},
   "source": [
    "### 5.2) Embed text only representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "behind-snake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29458, 300)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = text_only_df['processed_tweet'].values\n",
    "vectors_array = vectorize_tweets(tweets_list)\n",
    "vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "assured-reproduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 310)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "dimensions = create_dimensions_array()\n",
    "text_only_vectors = pd.DataFrame(vectors_array, columns=dimensions)\n",
    "text_only_vectors = add_additional_information(text_only_vectors)\n",
    "print(text_only_vectors.shape)\n",
    "print(text_only_vectors['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-petroleum",
   "metadata": {},
   "source": [
    "### 5.3) Embed text + hashtags representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cosmetic-plymouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29458, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = text_and_hashtags_df['processed_tweet'].values\n",
    "vectors_array = vectorize_tweets(tweets_list)\n",
    "vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fifteen-bumper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 310)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "dimensions = create_dimensions_array()\n",
    "text_and_hashtags_vectors = pd.DataFrame(vectors_array, columns=dimensions)\n",
    "text_and_hashtags_vectors = add_additional_information(text_and_hashtags_vectors)\n",
    "print(text_and_hashtags_vectors.shape)\n",
    "print(text_and_hashtags_vectors['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-bunny",
   "metadata": {},
   "source": [
    "### 5.4) Embed hybrid representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "mobile-leonard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29458, 300)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = hybrid_df['processed_tweet'].values\n",
    "vectors_array = vectorize_tweets(tweets_list)\n",
    "vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "oriented-checkout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29458, 310)\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "dimensions = create_dimensions_array()\n",
    "hybrid_vectors = pd.DataFrame(vectors_array, columns=dimensions)\n",
    "hybrid_vectors = add_additional_information(hybrid_vectors)\n",
    "print(hybrid_vectors.shape)\n",
    "print(hybrid_vectors['zip_code'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-failure",
   "metadata": {},
   "source": [
    "# 6) Save dataframes as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "south-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_vectors.to_csv('text_only_tweets.csv', index=False)\n",
    "text_and_hashtags_vectors.to_csv('text_and_hashtags_tweets.csv', index=False)\n",
    "hybrid_vectors.to_csv('hybrid_tweets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
