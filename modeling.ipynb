{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "theoretical-regard",
   "metadata": {},
   "source": [
    "# Data Analysis + Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-roommate",
   "metadata": {},
   "source": [
    "It is assumed that a master csv file (named \"master_individual.csv\") exists containing the following information for each individual tweet:\n",
    "- tweet ID, full text, sentiment score, date, hashtags\n",
    "- location of origin data (city, state, place type, zip code, metropolitan area)\n",
    "- data for the zip code from which the tweet originates (average Zillow Home Value Index (ZHVI), number of establishments in educational services, number of establishments in healthcare and social assistance, number of establishments in professional, scientific, and technical services, ground truth vaccine hesitancy, binarized ground truth vaccine hesitancy)\n",
    "\n",
    "It is also assumed that a master csv file (named \"master_aggregate.csv\") exists containing the following information for each zip code:\n",
    "- average and standard deviation of sentiment across all tweets originating from that zip code\n",
    "- metropolitan area in which zip code is located\n",
    "- zip code-level data (average Zillow Home Value Index (ZHVI), number of establishments in educational services, number of establishments in healthcare and social assistance, number of establishments in professional, scientific, and technical services, ground truth vaccine hesitancy, binarized ground truth vaccine hesitancy)\n",
    "\n",
    "Note: When binarizing the ground truth vaccine hesitancy for each zip code, we used 0.70 as the cut-off (i.e. a continuous ground truth vaccine hesitancy of >= 0.70 corresponds to a 1 and all other cases results in 0). However, binarized ground truth vaccine hesitancy is ultimately not used in study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southern-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "norwegian-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_individual = pd.read_csv('master_individual.csv')\n",
    "master_aggregate = pd.read_csv('master_aggregate.csv')\n",
    "text_only_tweets = pd.read_csv('text_only_tweets.csv')\n",
    "text_and_hashtags_tweets = pd.read_csv('text_and_hashtags_tweets.csv')\n",
    "hybrid_tweets = pd.read_csv('hybrid_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-mediterranean",
   "metadata": {},
   "source": [
    "# 1) Create ground truth vaccine hesitancy dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-chile",
   "metadata": {},
   "source": [
    "This dataframe contains all the zip codes present in master_individual with their corresponding vaccine hesitancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "relevant-allowance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_vac_hes_per_zip_code = pd.DataFrame()\n",
    "baseline_vac_hes_per_zip_code['zip_code'] = master_aggregate['zip_code'].values\n",
    "baseline_vac_hes_per_zip_code['vac_hes'] = master_aggregate['vac_hes'].values\n",
    "baseline_vac_hes_per_zip_code['vac_hes_bin'] = master_aggregate['vac_hes_bin'].values\n",
    "baseline_vac_hes_per_zip_code['metropolitan_area'] = master_aggregate['metropolitan_area'].values\n",
    "baseline_vac_hes_per_zip_code = baseline_vac_hes_per_zip_code.sort_values(by=['zip_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indie-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max vaccine hesitancy: 1.0\n",
      "Min vaccine hesitancy: 0.0\n",
      "Std vaccine hesitancy: 0.3343627664992047\n",
      "Mean vaccine hesitancy: 0.24025403264754172\n"
     ]
    }
   ],
   "source": [
    "print('Max vaccine hesitancy:', max(baseline_vac_hes_per_zip_code['vac_hes']))\n",
    "print('Min vaccine hesitancy:', min(baseline_vac_hes_per_zip_code['vac_hes']))\n",
    "print('Std vaccine hesitancy:', baseline_vac_hes_per_zip_code['vac_hes'].std())\n",
    "print('Mean vaccine hesitancy:', baseline_vac_hes_per_zip_code['vac_hes'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-jumping",
   "metadata": {},
   "source": [
    "# 2) Create stratified split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-harvey",
   "metadata": {},
   "source": [
    "It is necessary that that the tweets (tweet IDs) in the test set are the same across all text representations (text only, text and hashtags, and hybrid) in order to be able to compare predictive power of the models. Training will be done on different parts of the tweet (depending on the representation used), but but the testing should be done on the same tweet IDs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-suspension",
   "metadata": {},
   "source": [
    "### 2.1) Text only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "trying-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_tweets_copy = text_only_tweets.copy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(text_only_tweets_copy, text_only_tweets_copy['zip_code']):\n",
    "    train_text_only = text_only_tweets_copy.iloc[train_index]\n",
    "    test_text_only = text_only_tweets_copy.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "average-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "493\n",
      "\n",
      "NewYork         0.428074\n",
      "LosAngeles      0.357464\n",
      "Chicago         0.052406\n",
      "Houston         0.051897\n",
      "SanDiego        0.035942\n",
      "Philadelphia    0.027837\n",
      "Dallas          0.022575\n",
      "Phoenix         0.016846\n",
      "SanAntonio      0.006959\n",
      "Name: metropolitan_area, dtype: float64\n",
      "NewYork         0.428377\n",
      "LosAngeles      0.357773\n",
      "Chicago         0.052444\n",
      "Houston         0.051935\n",
      "SanDiego        0.036320\n",
      "Philadelphia    0.027325\n",
      "Dallas          0.022403\n",
      "Phoenix         0.016802\n",
      "SanAntonio      0.006619\n",
      "Name: metropolitan_area, dtype: float64\n",
      "\n",
      "(23566, 310)\n",
      "(5892, 310)\n"
     ]
    }
   ],
   "source": [
    "train_text_only.reset_index(inplace=True, drop=True)\n",
    "test_text_only.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(train_text_only['zip_code'].nunique())\n",
    "print(test_text_only['zip_code'].nunique())\n",
    "print()\n",
    "print(train_text_only['metropolitan_area'].value_counts() / len(train_text_only))\n",
    "print(test_text_only['metropolitan_area'].value_counts() / len(test_text_only))\n",
    "print()\n",
    "print(train_text_only.shape)\n",
    "print(test_text_only.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-chase",
   "metadata": {},
   "source": [
    "### 2.2) Text + hashtags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "separate-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_hashtags_tweets_copy = text_and_hashtags_tweets.copy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(text_and_hashtags_tweets_copy, text_and_hashtags_tweets_copy['zip_code']):\n",
    "    train_text_and_hashtags = text_and_hashtags_tweets_copy.iloc[train_index]\n",
    "    test_text_and_hashtags = text_and_hashtags_tweets_copy.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accomplished-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "493\n",
      "\n",
      "NewYork         0.428074\n",
      "LosAngeles      0.357464\n",
      "Chicago         0.052406\n",
      "Houston         0.051897\n",
      "SanDiego        0.035942\n",
      "Philadelphia    0.027837\n",
      "Dallas          0.022575\n",
      "Phoenix         0.016846\n",
      "SanAntonio      0.006959\n",
      "Name: metropolitan_area, dtype: float64\n",
      "NewYork         0.428377\n",
      "LosAngeles      0.357773\n",
      "Chicago         0.052444\n",
      "Houston         0.051935\n",
      "SanDiego        0.036320\n",
      "Philadelphia    0.027325\n",
      "Dallas          0.022403\n",
      "Phoenix         0.016802\n",
      "SanAntonio      0.006619\n",
      "Name: metropolitan_area, dtype: float64\n",
      "\n",
      "(23566, 310)\n",
      "(5892, 310)\n"
     ]
    }
   ],
   "source": [
    "train_text_and_hashtags.reset_index(inplace=True, drop=True)\n",
    "test_text_and_hashtags.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(train_text_and_hashtags['zip_code'].nunique())\n",
    "print(test_text_and_hashtags['zip_code'].nunique())\n",
    "print()\n",
    "print(train_text_and_hashtags['metropolitan_area'].value_counts() / len(train_text_and_hashtags))\n",
    "print(test_text_and_hashtags['metropolitan_area'].value_counts() / len(test_text_and_hashtags))\n",
    "print()\n",
    "print(train_text_and_hashtags.shape)\n",
    "print(test_text_and_hashtags.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-bundle",
   "metadata": {},
   "source": [
    "### 2.3) Hybrid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "drawn-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_tweets_copy = hybrid_tweets.copy()\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(hybrid_tweets_copy, hybrid_tweets_copy['zip_code']):\n",
    "    train_hybrid = hybrid_tweets_copy.iloc[train_index]\n",
    "    test_hybrid = hybrid_tweets_copy.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "finite-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "493\n",
      "\n",
      "NewYork         0.428074\n",
      "LosAngeles      0.357464\n",
      "Chicago         0.052406\n",
      "Houston         0.051897\n",
      "SanDiego        0.035942\n",
      "Philadelphia    0.027837\n",
      "Dallas          0.022575\n",
      "Phoenix         0.016846\n",
      "SanAntonio      0.006959\n",
      "Name: metropolitan_area, dtype: float64\n",
      "NewYork         0.428377\n",
      "LosAngeles      0.357773\n",
      "Chicago         0.052444\n",
      "Houston         0.051935\n",
      "SanDiego        0.036320\n",
      "Philadelphia    0.027325\n",
      "Dallas          0.022403\n",
      "Phoenix         0.016802\n",
      "SanAntonio      0.006619\n",
      "Name: metropolitan_area, dtype: float64\n",
      "\n",
      "(23566, 310)\n",
      "(5892, 310)\n"
     ]
    }
   ],
   "source": [
    "train_hybrid.reset_index(inplace=True, drop=True)\n",
    "test_hybrid.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(train_hybrid['zip_code'].nunique())\n",
    "print(test_hybrid['zip_code'].nunique())\n",
    "print()\n",
    "print(train_hybrid['metropolitan_area'].value_counts() / len(train_hybrid))\n",
    "print(test_hybrid['metropolitan_area'].value_counts() / len(test_hybrid))\n",
    "print()\n",
    "print(train_hybrid.shape)\n",
    "print(test_hybrid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-bathroom",
   "metadata": {},
   "source": [
    "### 2.4) Dataframe summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-lemon",
   "metadata": {},
   "source": [
    "At this point, there are 3 types of dataframes:\n",
    "- text only (train_text_only and test_text_only)\n",
    "- text and hashtags (train_text_and_hashtags and test_text_and_hashtags)\n",
    "- hybrid (train_hybrid, test_hybrid)\n",
    "\n",
    "The train and test sets contain the same tweet IDs, but not the same embedded information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-monitoring",
   "metadata": {},
   "source": [
    "# 3) Create Baseline Predictions Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-observation",
   "metadata": {},
   "source": [
    "### 3.1) Calculate mean vaccine hesitancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "earned-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean vac hes in text only: 0.23486475768029166\n",
      "Mean vac hes in text and hashtags: 0.23486475768029166\n",
      "Mean vac hes in hybrid: 0.23486475768029166\n",
      "\n",
      "Mean vac hes in baseline vaccine hesitancy: 0.24025403264754172\n",
      "\n",
      "Mean vac hes in train text only: 0.23490204478068344\n",
      "Mean vac hes in train text and hashtags: 0.23490204478068344\n",
      "Mean vac hes in train hybrid: 0.23490204478068344\n"
     ]
    }
   ],
   "source": [
    "mean_text_only_tweets = text_only_tweets['vac_hes'].mean()\n",
    "mean_text_and_hashtags_tweets = text_and_hashtags_tweets['vac_hes'].mean()\n",
    "mean_hybrid_tweets = hybrid_tweets['vac_hes'].mean()\n",
    "mean_complete_dataset = mean_text_only_tweets\n",
    "\n",
    "mean_baseline_vac_hes = baseline_vac_hes_per_zip_code['vac_hes'].mean()\n",
    "\n",
    "mean_train_text_only = train_text_only['vac_hes'].mean()\n",
    "mean_train_text_and_hashtags = train_text_and_hashtags['vac_hes'].mean()\n",
    "mean_train_hybrid = train_hybrid['vac_hes'].mean()\n",
    "mean_train_set = mean_train_text_only\n",
    "\n",
    "print('Mean vac hes in text only:', mean_text_only_tweets)\n",
    "print('Mean vac hes in text and hashtags:', mean_text_and_hashtags_tweets)\n",
    "print('Mean vac hes in hybrid:', mean_hybrid_tweets)\n",
    "print()\n",
    "print('Mean vac hes in baseline vaccine hesitancy:', mean_baseline_vac_hes)\n",
    "print()\n",
    "print('Mean vac hes in train text only:', mean_train_text_only)\n",
    "print('Mean vac hes in train text and hashtags:', mean_train_text_and_hashtags)\n",
    "print('Mean vac hes in train hybrid:', mean_train_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-heater",
   "metadata": {},
   "source": [
    "### 3.2) Create predictions arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "outside-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5892 5892 5892\n",
      "pred_size: 5892\n"
     ]
    }
   ],
   "source": [
    "print(len(test_text_only), len(test_text_and_hashtags), len(test_hybrid))\n",
    "pred_size = len(test_text_only)\n",
    "print('pred_size:', pred_size)\n",
    "\n",
    "pred_all_zero = np.full(pred_size, 0)\n",
    "pred_all_one = np.full(pred_size, 1)\n",
    "pred_all_half = np.full(pred_size, 0.5)\n",
    "pred_mean_train_set = np.full(pred_size, mean_train_set)\n",
    "pred_mean_entire_dataset = np.full(pred_size, mean_complete_dataset)\n",
    "pred_mean_baseline_vac_hes = np.full(pred_size, mean_baseline_vac_hes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-clear",
   "metadata": {},
   "source": [
    "# 4) Calculate Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nervous-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(true_values, pred):\n",
    "    return np.sqrt(np.mean((pred-true_values)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-passenger",
   "metadata": {},
   "source": [
    "### 4.1) Individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "talented-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE when predictions are all zero: 0.39881213912013264 and 0.39881213912013264\n",
      "Baseline RMSE when predictions are all one: 0.8304335484783919 and 0.8304335484783919\n",
      "Baseline RMSE when predictions are all 0.5: 0.4175350289185266 and 0.4175350289185266\n",
      "Baseline RMSE when predictions are all mean of train set: 0.3224278739229078 and 0.3224278739229078\n",
      "Baseline RMSE when predictions are all mean of entire dataset: 0.32242785452010764 and 0.32242785452010764\n",
      "Baseline RMSE when predictions are all mean of baseline vac hes: 0.32247538374608337 and 0.32247538374608337\n"
     ]
    }
   ],
   "source": [
    "test_labels = test_text_only['vac_hes'].copy()\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_all_zero)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_all_zero))\n",
    "print('Baseline RMSE when predictions are all zero:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_all_one)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_all_one))\n",
    "print('Baseline RMSE when predictions are all one:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_all_half)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_all_half))\n",
    "print('Baseline RMSE when predictions are all 0.5:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_mean_train_set)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_mean_train_set))\n",
    "print('Baseline RMSE when predictions are all mean of train set:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_mean_entire_dataset)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_mean_entire_dataset))\n",
    "print('Baseline RMSE when predictions are all mean of entire dataset:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(test_labels, pred_mean_baseline_vac_hes)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_labels, pred_mean_baseline_vac_hes))\n",
    "print('Baseline RMSE when predictions are all mean of baseline vac hes:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-opera",
   "metadata": {},
   "source": [
    "### 4.2) Aggregated by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cheap-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_size: 493\n"
     ]
    }
   ],
   "source": [
    "pred_size = len(baseline_vac_hes_per_zip_code)\n",
    "print('pred_size:', pred_size)\n",
    "\n",
    "pred_all_zero = np.full(pred_size, 0)\n",
    "pred_all_one = np.full(pred_size, 1)\n",
    "pred_all_half = np.full(pred_size, 0.5)\n",
    "pred_mean_train_set = np.full(pred_size, mean_train_set)\n",
    "pred_mean_entire_dataset = np.full(pred_size, mean_complete_dataset)\n",
    "pred_mean_baseline_vac_hes = np.full(pred_size, mean_baseline_vac_hes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "peaceful-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE when predictions are all zero: 0.4114531420478382 and 0.4114531420478382\n",
      "Baseline RMSE when predictions are all one: 0.8299310952157144 and 0.8299310952157144\n",
      "Baseline RMSE when predictions are all 0.5: 0.42313077819215283 and 0.42313077819215283\n",
      "Baseline RMSE when predictions are all mean of train set: 0.33406635818615366 and 0.33406635818615366\n",
      "Baseline RMSE when predictions are all mean of entire dataset: 0.3340669576332044 and 0.3340669576332044\n",
      "Baseline RMSE when predictions are all mean of baseline vac hes: 0.3340234840510956 and 0.3340234840510956\n"
     ]
    }
   ],
   "source": [
    "labels = baseline_vac_hes_per_zip_code['vac_hes']\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_all_zero)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_all_zero))\n",
    "print('Baseline RMSE when predictions are all zero:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_all_one)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_all_one))\n",
    "print('Baseline RMSE when predictions are all one:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_all_half)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_all_half))\n",
    "print('Baseline RMSE when predictions are all 0.5:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_mean_train_set)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_mean_train_set))\n",
    "print('Baseline RMSE when predictions are all mean of train set:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_mean_entire_dataset)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_mean_entire_dataset))\n",
    "print('Baseline RMSE when predictions are all mean of entire dataset:', rmse, 'and', rmse2)\n",
    "\n",
    "rmse = calculate_rmse(labels, pred_mean_baseline_vac_hes)\n",
    "rmse2 = np.sqrt(mean_squared_error(labels, pred_mean_baseline_vac_hes))\n",
    "print('Baseline RMSE when predictions are all mean of baseline vac hes:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-corner",
   "metadata": {},
   "source": [
    "# 5) Standardize numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "radical-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split up dataframe into columns that need to be standardized\n",
    "# and those that do not\n",
    "def split_dataframe(df):\n",
    "    # columns to be standardized\n",
    "    col_names = ['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                'num_est_prof_sci_tech_serv','sentiment']\n",
    "    df_std = df[col_names]\n",
    "    df_no_change = df.drop(columns=col_names)\n",
    "    return df_std, df_no_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "existing-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standardize certain columns in dataframe\n",
    "def standardize(df):\n",
    "    # columns to be standardized\n",
    "    col_names = ['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                'num_est_prof_sci_tech_serv','sentiment']\n",
    "    df_std = StandardScaler().fit_transform(df)\n",
    "    df_std = pd.DataFrame(df_std, columns=col_names)\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-cross",
   "metadata": {},
   "source": [
    "### 5.1) Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "architectural-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_only_copy = train_text_only.copy()\n",
    "train_text_only_std, train_text_only_no_change = split_dataframe(train_text_only_copy)\n",
    "train_text_only_std = standardize(train_text_only_std)\n",
    "train_text_only = train_text_only_no_change.join(train_text_only_std)\n",
    "\n",
    "test_text_only_copy = test_text_only.copy()\n",
    "test_text_only_std, test_text_only_no_change = split_dataframe(test_text_only_copy)\n",
    "test_text_only_std = standardize(test_text_only_std)\n",
    "test_text_only = test_text_only_no_change.join(test_text_only_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-computer",
   "metadata": {},
   "source": [
    "### 5.2) Text + hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "attached-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_and_hashtags_copy = train_text_and_hashtags.copy()\n",
    "train_text_and_hashtags_std, train_text_and_hashtags_no_change = split_dataframe(train_text_and_hashtags_copy)\n",
    "train_text_and_hashtags_std = standardize(train_text_and_hashtags_std)\n",
    "train_text_and_hashtags = train_text_and_hashtags_no_change.join(train_text_and_hashtags_std)\n",
    "\n",
    "test_text_and_hashtags_copy = test_text_and_hashtags.copy()\n",
    "test_text_and_hashtags_std, test_text_and_hashtags_no_change = split_dataframe(test_text_and_hashtags_copy)\n",
    "test_text_and_hashtags_std = standardize(test_text_and_hashtags_std)\n",
    "test_text_and_hashtags = test_text_and_hashtags_no_change.join(test_text_and_hashtags_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-appeal",
   "metadata": {},
   "source": [
    "### 5.3) Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rental-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hybrid_copy = train_hybrid.copy()\n",
    "train_hybrid_std, train_hybrid_no_change = split_dataframe(train_hybrid_copy)\n",
    "train_hybrid_std = standardize(train_hybrid_std)\n",
    "train_hybrid = train_hybrid_no_change.join(train_hybrid_std)\n",
    "\n",
    "test_hybrid_copy = test_hybrid.copy()\n",
    "test_hybrid_std, test_hybrid_no_change = split_dataframe(test_hybrid_copy)\n",
    "test_hybrid_std = standardize(test_hybrid_std)\n",
    "test_hybrid = test_text_only_no_change.join(test_hybrid_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-dress",
   "metadata": {},
   "source": [
    "# 6) Build baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-bowling",
   "metadata": {},
   "source": [
    "Model input data can be adjusted by droppping appropriate columns in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "architectural-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_only_copy = train_text_only.copy()\n",
    "# train_input = train_text_only_copy.drop(columns=['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "#                                                 'num_est_prof_sci_tech_serv', 'sentiment', 'id','zip_code',\n",
    "#                                                 'metropolitan_area','vac_hes','vac_hes_bin'])\n",
    "train_input = train_text_only_copy[['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                    'num_est_prof_sci_tech_serv', 'sentiment']]\n",
    "\n",
    "train_labels = train_text_only_copy['vac_hes']\n",
    "\n",
    "test_text_only_copy = test_text_only.copy()\n",
    "# test_input = test_text_only_copy.drop(columns=['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "#                                                 'num_est_prof_sci_tech_serv', 'sentiment','id','zip_code',\n",
    "#                                                 'metropolitan_area','vac_hes','vac_hes_bin'])\n",
    "test_input = test_text_only_copy[['avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                    'num_est_prof_sci_tech_serv', 'sentiment']]\n",
    "test_labels = test_text_only_copy['vac_hes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-while",
   "metadata": {},
   "source": [
    "Model type can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "suspended-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation scores: [-0.3065724  -0.3124583  -0.30723302 -0.30314838 -0.31227854]\n",
      "average cross validation score: -0.308338127397242\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "# model = SVR(kernel='rbf')\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, train_input, train_labels, scoring='neg_root_mean_squared_error', cv=folds)\n",
    "print('cross validation scores:', scores)\n",
    "print('average cross validation score:', scores.mean())\n",
    "\n",
    "model = model.fit(train_input, train_labels)\n",
    "model_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-colon",
   "metadata": {},
   "source": [
    "Calculate tweet-level RMSE two ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "phantom-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.30793025992382783\n",
      "RMSE: 0.30793025992382783\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_labels, model_pred)\n",
    "print('RMSE:', rmse)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, model_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-visitor",
   "metadata": {},
   "source": [
    "Calculate zip code-level RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "scheduled-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(493, 2)\n",
      "RMSE of predictions grouped by zip code: 0.3445654066728429 and 0.3445654066728429\n"
     ]
    }
   ],
   "source": [
    "model_pred_df = pd.DataFrame(model_pred, columns=['vac_hes_pred'])\n",
    "model_pred_df['zip_code'] = test_text_only['zip_code']\n",
    "\n",
    "# dataframe with all the predictions grouped by zip code (taking average of predictions)\n",
    "model_pred_df_zip_code_mean = model_pred_df.groupby(['zip_code'], as_index=False).mean()\n",
    "model_pred_df_zip_code_mean = model_pred_df_zip_code_mean.sort_values(by=['zip_code'])\n",
    "print(model_pred_df_zip_code_mean.shape)\n",
    "\n",
    "# the baseline dataframe is already sorted by zip code\n",
    "# now, both the model predictions and the baseline vaccine hesitancies are sorted by zip code\n",
    "true_labels = baseline_vac_hes_per_zip_code['vac_hes']\n",
    "pred = model_pred_df_zip_code_mean['vac_hes_pred']\n",
    "\n",
    "rmse = calculate_rmse(true_labels, pred)\n",
    "rmse2 = np.sqrt(mean_squared_error(true_labels, pred))\n",
    "print('RMSE of predictions grouped by zip code:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-innocent",
   "metadata": {},
   "source": [
    "# 7) Build Machine Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-dealing",
   "metadata": {},
   "source": [
    "### 7.1) Text only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-garlic",
   "metadata": {},
   "source": [
    "Model input data can be adjusted by droppping appropriate columns in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "opposed-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input = train_text_only.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "#                                                     'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "#                                                     'num_est_prof_sci_tech_serv', 'sentiment'])\n",
    "\n",
    "train_input = train_text_only.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'sentiment'])\n",
    "\n",
    "\n",
    "train_labels = train_text_only['vac_hes'].copy()\n",
    "\n",
    "# test_input = test_text_only.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "#                                                     'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "#                                                     'num_est_prof_sci_tech_serv', 'sentiment'])\n",
    "\n",
    "test_input = test_text_only.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'sentiment'])\n",
    "\n",
    "\n",
    "test_labels = test_text_only['vac_hes'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-anderson",
   "metadata": {},
   "source": [
    "Model type can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "athletic-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation scores: [-0.20539328 -0.2127378  -0.21148047 -0.20088998 -0.21352056]\n",
      "average cross validation score: -0.2088044167729281\n"
     ]
    }
   ],
   "source": [
    "# model = LinearRegression()\n",
    "# model = LinearSVR(max_iter=4000, random_state=42)\n",
    "# model = SGDRegressor(random_state=42)\n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, train_input, train_labels, scoring='neg_root_mean_squared_error', cv=folds)\n",
    "print('cross validation scores:', scores)\n",
    "print('average cross validation score:', scores.mean())\n",
    "\n",
    "model = model.fit(train_input, train_labels)\n",
    "model_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-union",
   "metadata": {},
   "source": [
    "#### 7.1.1) Compute metrics on individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "gentle-transportation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.20624460462988284\n",
      "RMSE: 0.20624460462988284\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_labels, model_pred)\n",
    "print('RMSE:', rmse)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, model_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-insertion",
   "metadata": {},
   "source": [
    "#### 7.1.2) Compute metrics on predictions aggregated by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "enabling-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.DataFrame(model_pred, columns=['vac_hes_pred'])\n",
    "model_pred_df['zip_code'] = test_text_only['zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "increased-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all the predictions grouped by zip code (taking average of predictions)\n",
    "model_pred_df_zip_code_mean = model_pred_df.groupby(['zip_code'], as_index=False).mean()\n",
    "model_pred_df_zip_code_mean = model_pred_df_zip_code_mean.sort_values(by=['zip_code'])\n",
    "model_pred_df_zip_code_mean['metropolitan_area'] = baseline_vac_hes_per_zip_code['metropolitan_area']\n",
    "model_pred_df_zip_code_mean['vac_hes_true'] = baseline_vac_hes_per_zip_code['vac_hes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "knowing-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of predictions grouped by zip code: 0.3121243534981443 and 0.3121243534981443\n"
     ]
    }
   ],
   "source": [
    "# the baseline dataframe is already sorted by zip code\n",
    "# now, both the model predictions and the baseline vaccine hesitancies are sorted by zip code\n",
    "true_labels = baseline_vac_hes_per_zip_code['vac_hes']\n",
    "pred = model_pred_df_zip_code_mean['vac_hes_pred']\n",
    "\n",
    "rmse = calculate_rmse(true_labels, pred)\n",
    "rmse2 = np.sqrt(mean_squared_error(true_labels, pred))\n",
    "print('RMSE of predictions grouped by zip code:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-solution",
   "metadata": {},
   "source": [
    "### 7.2) Text + hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-setup",
   "metadata": {},
   "source": [
    "Model input data can be adjusted by droppping appropriate columns in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "excellent-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_text_and_hashtags.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                                    'num_est_prof_sci_tech_serv'])\n",
    "train_labels = train_text_and_hashtags['vac_hes'].copy()\n",
    "\n",
    "test_input = test_text_and_hashtags.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                                    'num_est_prof_sci_tech_serv'])\n",
    "test_labels = test_text_and_hashtags['vac_hes'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-craps",
   "metadata": {},
   "source": [
    "Model type can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "committed-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation scores: [-0.28644405 -0.29230373 -0.29031964 -0.28354353 -0.29331877]\n",
      "average cross validation score: -0.2891859422196078\n"
     ]
    }
   ],
   "source": [
    "# model = LinearRegression()\n",
    "# model = LinearSVR(max_iter=4000, random_state=42)\n",
    "# model = SGDRegressor(random_state=42)\n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, train_input, train_labels, scoring='neg_root_mean_squared_error', cv=folds)\n",
    "print('cross validation scores:', scores)\n",
    "print('average cross validation score:', scores.mean())\n",
    "\n",
    "model = model.fit(train_input, train_labels)\n",
    "model_pred = model.predict(test_input)\n",
    "\n",
    "# model = LinearRegression().fit(train_input, train_labels)\n",
    "# model_pred = model.predict(test_input)\n",
    "# print('total number of predictions:', len(model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-incidence",
   "metadata": {},
   "source": [
    "#### 7.2.1) Compute metrics on individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "soviet-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2906366503544305\n",
      "RMSE: 0.2906366503544305\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_labels, model_pred)\n",
    "print('RMSE:', rmse)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, model_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-triumph",
   "metadata": {},
   "source": [
    "#### 7.2.2) Compute metrics on predictions aggregated by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "loaded-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.DataFrame(model_pred, columns=['vac_hes_pred'])\n",
    "model_pred_df['zip_code'] = test_text_and_hashtags['zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "partial-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all the predictions grouped by zip code (taking average of predictions)\n",
    "model_pred_df_zip_code_mean = model_pred_df.groupby(['zip_code'], as_index=False).mean()\n",
    "model_pred_df_zip_code_mean = model_pred_df_zip_code_mean.sort_values(by=['zip_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "integrated-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of predictions grouped by zip code: 0.3345441658461021 and 0.3345441658461021\n"
     ]
    }
   ],
   "source": [
    "# the baseline dataframe is already sorted by zip code\n",
    "# now, both the model predictions and the baseline vaccine hesitancies are sorted by zip code\n",
    "true_labels = baseline_vac_hes_per_zip_code['vac_hes']\n",
    "pred = model_pred_df_zip_code_mean['vac_hes_pred']\n",
    "\n",
    "rmse = calculate_rmse(true_labels, pred)\n",
    "rmse2 = np.sqrt(mean_squared_error(true_labels, pred))\n",
    "print('RMSE of predictions grouped by zip code:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-engineering",
   "metadata": {},
   "source": [
    "### 7.3) Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-stream",
   "metadata": {},
   "source": [
    "Model input data can be adjusted by droppping appropriate columns in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "seven-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_hybrid.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                                    'num_est_prof_sci_tech_serv'])\n",
    "train_labels = train_hybrid['vac_hes'].copy()\n",
    "\n",
    "test_input = test_hybrid.copy().drop(columns=['vac_hes', 'metropolitan_area', 'vac_hes_bin',\n",
    "                                                    'zip_code', 'id', 'avg_zhvi','num_est_educ_serv','num_est_healthcare_social_assist',\n",
    "                                                    'num_est_prof_sci_tech_serv'])\n",
    "test_labels = test_hybrid['vac_hes'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-complexity",
   "metadata": {},
   "source": [
    "Model type can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "possible-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation scores: [-0.29570065 -0.299765   -0.29706717 -0.2896552  -0.30104773]\n",
      "average cross validation score: -0.2966471500790989\n"
     ]
    }
   ],
   "source": [
    "# model = LinearRegression()\n",
    "# model = LinearSVR(max_iter=4000, random_state=42)\n",
    "# model = SGDRegressor(random_state=42)\n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, train_input, train_labels, scoring='neg_root_mean_squared_error', cv=folds)\n",
    "print('cross validation scores:', scores)\n",
    "print('average cross validation score:', scores.mean())\n",
    "\n",
    "model = model.fit(train_input, train_labels)\n",
    "model_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-naples",
   "metadata": {},
   "source": [
    "#### 7.3.1) Compute metrics on individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "atlantic-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.29926414379679095\n",
      "RMSE: 0.29926414379679095\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_labels, model_pred)\n",
    "print('RMSE:', rmse)\n",
    "rmse = np.sqrt(mean_squared_error(test_labels, model_pred))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-haven",
   "metadata": {},
   "source": [
    "#### 7.3.2) Compute metrics on predictions aggregated by zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "differential-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_df = pd.DataFrame(model_pred, columns=['vac_hes_pred'])\n",
    "model_pred_df['zip_code'] = test_hybrid['zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "immune-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all the predictions grouped by zip code (taking average of predictions)\n",
    "model_pred_df_zip_code_mean = model_pred_df.groupby(['zip_code'], as_index=False).mean()\n",
    "model_pred_df_zip_code_mean = model_pred_df_zip_code_mean.sort_values(by=['zip_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eleven-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of predictions grouped by zip code: 0.33104134739951124 and 0.33104134739951124\n"
     ]
    }
   ],
   "source": [
    "# the baseline dataframe is already sorted by zip code\n",
    "# now, both the model predictions and the baseline vaccine hesitancies are sorted by zip code\n",
    "true_labels = baseline_vac_hes_per_zip_code['vac_hes']\n",
    "pred = model_pred_df_zip_code_mean['vac_hes_pred']\n",
    "\n",
    "rmse = calculate_rmse(true_labels, pred)\n",
    "rmse2 = np.sqrt(mean_squared_error(true_labels, pred))\n",
    "print('RMSE of predictions grouped by zip code:', rmse, 'and', rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-stick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
